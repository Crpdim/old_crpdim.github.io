---
title: torch求导
tags: 深度学习
mathjax: true
mode: immersive
comment: true
pageview: true
key: A0006
header:
  theme: dark
article_header:
  type: cover
  image:
    src: /Fly1.jpg

---



{:toc}

# PYTORCH求导

> 导数的计算几乎是所有深度学习优化算法的关键步骤，再深度学习中，通常会想选择模型参数可微的损失函数，也就是对于每个参数，向其增加或减少一个无穷小的量，就可以知道损失会以多大的速度增加或减少。

偏导：在深度学习中，函数通常依赖许多变量，所以，需要将微分的思想推广到多元函数上，也就是求对某变量的偏导。

梯度：连接一个多元函数的所有偏导数，就可以得到该函数的梯度向量。

## 自动微分

> 深度学习框架通过自动计算导数（自动微分）加快求导。

### 举个例子：对函数𝑦=2**𝐱**<sup>⊤</sup>**𝐱**y=2x<sup>⊤</sup>x关于列向量**𝐱**求导

``` python
import torch
x = torch.arange(4.0)
#x = tensor([0., 1., 2., 3.])
```

这里创建变量x，并为他赋初值,要计算函数y关于x的梯度，就需要一块区域存放梯度，这样就不需要对每个参数求导时都分配内存。

```python
x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)
#y关于x的梯度会存放在x.gurd中
```

计算函数y：

```python
y = 2 * torch.dot(x, x) #y=2*（x的内积）
#计算得到y的标量：y = tensor(28., grad_fn=<MulBackward0>)
```

通过反向传播函数，得到y关于变量x每个分量的梯度

```python
y.backward() #反向传播函数
print(x.grad) #打印y关于x每个分量的梯度
#这里可以得到：tensor([ 0.,  4.,  8., 12.])
```

函数𝑦=2**𝐱**<sup>⊤</sup>**𝐱**关于**𝐱**的梯度应为4 * **𝐱**, 而4 * x = tensor([ 0.,  4.,  8., 12.])

所以可以得出梯度计算正确。

默认情况下，pytorch会累计梯度，所以此时若需要计算关于x的另一个函数，就需要清空之前计算的x的梯度

```python
x.grad.zero_() #清空梯度函数
#此时就可以计算其他函数关于x的梯度
```

### 对非标量变量的反向传播：y = x * x

当y不是标量时，y关于x的导数是一个矩阵。当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。

```python
y = x*x #此时y不是个标量，则需要计算批量样本的偏导数和
y.sum().backword()
print(x.grad) 
#可以得到： tensor([0., 2., 4., 6.]) 
```


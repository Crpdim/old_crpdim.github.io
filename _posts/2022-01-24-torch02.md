---
title: ä»é›¶å®ç°çº¿æ€§å›å½’
tags: æ·±åº¦å­¦ä¹ 
mathjax: true
mode: immersive
comment: true
pageview: true
key: A0007
header:
  theme: dark
article_header:
  type: cover
  image:
    src: /mountain1.jpg

---


* content
{:toc}


# çº¿æ€§å›å½’

> åœ¨å°è¯•å›«å›µåæ£å­¦ä¹ æ·±åº¦å­¦ä¹ åï¼Œæˆ‘æ·±åˆ‡åœ°è®¤è¯†åˆ°äº†è‡ªå·±çš„æµ®èºï¼Œè€—è´¹äº†ä¸å°‘æ—¶é—´ï¼Œå´å­¦åˆ°äº†å¯¥å¯¥æ— å‡ çš„é›¶æ•£çŸ¥è¯†ã€‚

## å›å½’ï¼ˆregressionï¼‰

ç ”ç©¶ä¸€ç»„éšæœºå˜é‡(Y1 ï¼ŒY2 ï¼Œâ€¦ï¼ŒYi)å’Œå¦ä¸€ç»„(X1ï¼ŒX2ï¼Œâ€¦ï¼ŒXk)å˜é‡ä¹‹é—´å…³ç³»çš„ç»Ÿè®¡åˆ†ææ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯èƒ½ä¸ºä¸€ä¸ªæˆ–å¤šä¸ªè‡ªå˜é‡ä¸å› å˜é‡ä¹‹é—´å…³ç³»å»ºæ¨¡çš„ä¸€ç±»æ–¹æ³•ã€‚ åœ¨è‡ªç„¶ç§‘å­¦å’Œç¤¾ä¼šç§‘å­¦é¢†åŸŸï¼Œå›å½’ç»å¸¸ç”¨æ¥è¡¨ç¤ºè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å…³ç³»ã€‚å½“æˆ‘ä»¬éœ€è¦é¢„æµ‹æ•°å€¼æ—¶ï¼Œä¾¿å¯èƒ½æ¶‰åŠåˆ°å›å½’é—®é¢˜ã€‚

## çº¿æ€§å›å½’

çº¿æ€§å›å½’åŸºäºå‡ ä¸ªç®€å•å‡è®¾

- è‡ªå˜é‡å’Œå› å˜é‡å…³ç³»æ˜¯çº¿æ€§çš„ï¼Œä¹Ÿå°±æ˜¯å› å˜é‡yå¯ä»¥è¡¨ç¤ºä¸ºè‡ªå˜é‡xä¸­å…ƒç´ çš„åŠ æƒå’Œã€‚ï¼ˆé€šå¸¸å¯ä»¥åŒ…å«è§‚æµ‹å€¼çš„ä¸€äº›å™ªå£°ã€‚
- å‡è®¾ä»»ä½•å™ªå£°éƒ½æ¯”è¾ƒæ­£å¸¸ï¼šå™ªå£°ç¬¦åˆæ­£æ€åˆ†å¸ƒã€‚

ä¹¦ä¸­ä¸¾çš„ä¾‹å­æ˜¯æˆ¿ä»·é¢„æµ‹ï¼šæ ¹æ®æˆ¿é¾„å’Œæˆ¿é—´é¢ç§¯ä¼°ç®—æˆ¿å±‹ä»·å€¼

é¦–å…ˆæˆ‘ä»¬éœ€è¦æ”¶é›†ä¸€ä¸ªçœŸå®çš„æ•°æ®é›†ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ç§°ä¸º**è®­ç»ƒé›†**ã€‚è®­ç»ƒé›†ä¸­ï¼Œæ¯è¡Œæ•°æ®è¢«ç§°ä¸º**æ ·æœ¬**æˆ–**æ•°æ®ç‚¹**ï¼Œä¹Ÿå°±æ˜¯ä¹‹å‰å‡ºå”®è¿‡çš„æˆ¿å±‹å±æ€§ï¼ˆæˆ¿é¾„ã€é¢ç§¯ï¼‰ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬æŠŠé¢„æµ‹çš„ç›®æ ‡ï¼ˆåœ¨è¿™ä¸ªä¾‹å­é‡Œæ˜¯æˆ¿ä»·ï¼‰ç§°ä¸º**æ ‡ç­¾ï¼ˆlabelï¼‰**æˆ–è€…**ç›®æ ‡ï¼ˆtargetï¼‰**ã€‚æŠŠè‡ªå˜é‡ï¼ˆæˆ¿é¾„ã€é¢ç§¯ï¼‰ç§°ä¸º**ç‰¹å¾ï¼ˆfeatureï¼‰**ã€‚

çº¿æ€§å‡è®¾æ˜¯æŒ‡ç›®æ ‡ï¼ˆæˆ¿å±‹ä»·æ ¼ï¼‰å¯ä»¥è¡¨ç¤ºä¸ºç‰¹å¾ï¼ˆé¢ç§¯å’Œæˆ¿é¾„ï¼‰çš„åŠ æƒå’Œã€‚å¦‚åœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯

**price=ğ‘¤areaâ‹…area+ğ‘¤ageâ‹…age+ğ‘.**

å…¶ä¸­ğ‘¤å°±æ˜¯æƒé‡ï¼Œå®ƒç›´æ¥å†³å®šäº†æ¯ä¸ªç‰¹å¾å¯¹æœ€ç»ˆé¢„æµ‹å€¼çš„å½±å“ã€‚bç§°ä¸ºåç§»é‡ï¼ˆåç½®ï¼ˆbiasï¼‰æˆ–æˆªè·ï¼‰ï¼Œå½“æ‰€æœ‰ç‰¹å¾éƒ½å–0æ—¶ï¼Œå…¶å†³å®šé¢„æµ‹å€¼åº”ä¸ºå¤šå°‘ã€‚

ç»™å®šè®­ç»ƒæ•°æ®ç‰¹å¾**ğ—**å’Œå¯¹åº”çš„å·²çŸ¥æ ‡ç­¾**ğ²**ï¼Œ çº¿æ€§å›å½’çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„æƒé‡å‘é‡**ğ°**å’Œåç½®ğ‘ã€‚åœ¨å¼€å§‹å¯»æ‰¾æœ€å¥½çš„*æ¨¡å‹å‚æ•°*ï¼ˆmodel parametersï¼‰**ğ°**å’Œğ‘ä¹‹å‰ï¼Œ æˆ‘ä»¬è¿˜éœ€è¦ä¸¤ä¸ªä¸œè¥¿ï¼š 

- ä¸€ç§æ¨¡å‹è´¨é‡çš„åº¦é‡æ–¹å¼(æŸå¤±å‡½æ•°ï¼‰ï¼› 
- ä¸€ç§èƒ½å¤Ÿæ›´æ–°æ¨¡å‹ä»¥æé«˜æ¨¡å‹é¢„æµ‹è´¨é‡çš„æ–¹æ³•ï¼ˆæ¨¡å‹ä¼˜åŒ–ï¼‰ã€‚

## æŸå¤±å‡½æ•°

> *æŸå¤±å‡½æ•°*ï¼ˆloss functionï¼‰èƒ½å¤Ÿé‡åŒ–ç›®æ ‡çš„*å®é™…*å€¼ä¸*é¢„æµ‹*å€¼ä¹‹é—´çš„å·®è·ã€‚ é€šå¸¸æˆ‘ä»¬ä¼šé€‰æ‹©éè´Ÿæ•°ä½œä¸ºæŸå¤±ï¼Œä¸”æ•°å€¼è¶Šå°è¡¨ç¤ºæŸå¤±è¶Šå°ï¼Œå®Œç¾é¢„æµ‹æ—¶çš„æŸå¤±ä¸º0ã€‚å›å½’é—®é¢˜ä¸­æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å¹³æ–¹è¯¯å·®å‡½æ•°ã€‚

æ ·æœ¬l(i) çš„å¹³æ–¹è¯¯å·®å¯ä»¥å®šä¹‰ä¸º é¢„æµ‹å€¼å‡å»çœŸå®å€¼çš„å¹³æ–¹é™¤ä»¥äºŒã€‚

è‹¥åº¦é‡æ¨¡å‹åœ¨æ•´ä½“æ•°æ®é›†ä¸Šçš„è´¨é‡ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—è®­ç»ƒé›†nä¸ªæ ·æœ¬ä¸Šçš„æŸå¤±å‡å€¼

```
def squared_loss(y_hat,y): #æŸå¤±å‡½æ•°å®ç°
    #y_hat ï¼š é¢„æµ‹å€¼
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

## æ¨¡å‹ä¼˜åŒ–

è§£æè§£ï¼šçº¿æ€§å›å½’çš„è§£å¯ä»¥ç”¨ä¸€ä¸ªå…¬å¼ç®€å•åœ°è¡¨è¾¾å‡ºæ¥ï¼Œ è¿™ç±»è§£å«ä½œè§£æè§£,ä½†å¹¶ä¸æ˜¯æ‰€æœ‰çš„é—®é¢˜éƒ½å­˜åœ¨è§£æè§£ã€‚ è§£æè§£å¯ä»¥è¿›è¡Œå¾ˆå¥½çš„æ•°å­¦åˆ†æï¼Œä½†è§£æè§£å¯¹é—®é¢˜çš„é™åˆ¶å¾ˆä¸¥æ ¼ï¼Œå¯¼è‡´å®ƒæ— æ³•å¹¿æ³›åº”ç”¨åœ¨æ·±åº¦å­¦ä¹ é‡Œã€‚

å³ä½¿åœ¨æ— æ³•å¾—åˆ°è§£æè§£çš„æƒ…å†µï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥æœ‰æ•ˆåœ°è®­ç»ƒæ¨¡å‹ã€‚ åœ¨è®¸å¤šä»»åŠ¡ä¸Šï¼Œé‚£äº›éš¾ä»¥ä¼˜åŒ–çš„æ¨¡å‹æ•ˆæœè¦æ›´å¥½ã€‚ä¹¦ä¸­ç”¨åˆ°çš„æ˜¯æ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•å‡ ä¹å¯ä»¥ä¼˜åŒ–æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

ç®—æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼š 

- åˆå§‹åŒ–æ¨¡å‹å‚æ•°çš„å€¼ï¼Œå¦‚éšæœºåˆå§‹åŒ–ï¼›
- ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–å°æ‰¹é‡æ ·æœ¬ä¸”åœ¨è´Ÿæ¢¯åº¦çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°ï¼Œå¹¶ä¸æ–­è¿­ä»£è¿™ä¸€æ­¥éª¤ã€‚

```
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

## çº¿æ€§å›å½’ä»é›¶å¼€å§‹å®ç°

### æ•°æ®é›†ç”Ÿæˆï¼š

æ„é€ äººé€ æ•°æ®é›†ï¼š

```
import random
from numpy import indices
import torch
from d2l import torch as d2l
def syn(w,b,num_example):              #æ„é€ æ•°æ®é›†å‡½æ•°
    x = torch.normal(0,1,(num_example,len(w))) #å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼Œnä¸ªæ ·æœ¬ï¼Œwé•¿åº¦ä¸ªåˆ—
    y = torch.matmul(x,w) + b         #ç”ŸæˆçœŸå®æ•°æ®y
    y += torch.normal(0,0.01,y.shape) #åƒæ•°æ®ä¸­åŠ å…¥å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º0.1çš„éšæœºå™ªå£°
    return x,y.reshape((-1,1))

true_w = torch.tensor([2,-3.4])   #çœŸå®w
true_b = 4.2                      #çœŸå®b
features,labels = syn(true_w,true_b,1000) #ç”Ÿæˆä¸€åƒæ¡è®­ç»ƒæ•°æ®ï¼Œå¾—åˆ°featyresï¼Œlabels
```

### è¯»å–æ•°æ®é›†ï¼š

```
def data_iter(batch_size,feature,labels):   #æ‰¹é‡è¯»å–æ ·æœ¬
    #æ‰“ä¹±æ•°æ®é›†ä¸­çš„æ ·æœ¬å¹¶ä»¥å°æ‰¹é‡æ–¹å¼è·å–æ•°æ®
    num_example = len(feature)              #æ ·æœ¬æ•°é‡
    indices = list(range(num_example))      
    random.shuffle(indices)                 #éšæœºè¯»å–
    for i in range(0,num_example,batch_size):
        batch_indices = torch.tensor(indices[i:
            min(i + batch_size, num_example)])
        yield feature[batch_indices],labels[batch_indices]

```

### åˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼š

```
#ä»å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºæ•°æ¥åˆå§‹åŒ–æƒé‡ï¼Œ å¹¶å°†åç½®åˆå§‹åŒ–ä¸º0
w = torch.normal(0,0.01,size = (2,1),requires_grad=True)
b = torch.zeros(1,requires_grad=True) #éœ€è¦å¯¹wå’Œbè¿›è¡Œæ›´æ–°ï¼Œrequires_frad = True
#åˆå§‹åŒ–å‚æ•°ä¹‹åï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ›´æ–°è¿™äº›å‚æ•°ï¼Œç›´åˆ°è¿™äº›å‚æ•°è¶³å¤Ÿæ‹Ÿåˆæˆ‘ä»¬çš„æ•°æ®ã€‚ æ¯æ¬¡æ›´æ–°éƒ½éœ€è¦è®¡ç®—æŸå¤±å‡½æ•°å…³äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚ æœ‰äº†è¿™ä¸ªæ¢¯åº¦ï¼Œæˆ‘ä»¬å°±å¯ä»¥å‘å‡å°æŸå¤±çš„æ–¹å‘æ›´æ–°æ¯ä¸ªå‚æ•°ã€‚
```

### å®šä¹‰æ¨¡å‹ï¼š

```
#çº¿æ€§å›å½’æ–¹ç¨‹
def linreg(X,w,b):
    return torch.matmul(X,w) + b
```

### å®šä¹‰æŸå¤±å‡½æ•°ï¼š

```
def squared_loss(y_hat,y):
    #y_hat ï¼š å®é™…å€¼
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

### å®šä¹‰ä¼˜åŒ–ç®—æ³•ï¼š

```
#å°æ‰¹é‡æ¢¯åº¦ä¸‹é™
#paramï¼šå‚æ•°
#lrï¼šå­¦ä¹ ç‡
#batchï¼š æ­¥é•¿
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            # print(param.grad)
            param.grad.zero_()
```

### å¼€å§‹è®­ç»ƒï¼š

```
lr = 0.03           #å­¦ä¹ ç‡
num_epochs = 3      #è®­ç»ƒæ¬¡æ•°
net = linreg        #å›å½’æ–¹ç¨‹
loss = squared_loss #å¹³æ–¹æŸå¤±å‡½æ•°

for epoch in range(num_epochs): 
    for X,y in data_iter(batchSize,features,labels):
        l = loss(net(X,w,b),y)  
        l.sum().backward() 			#å›å½’æ–¹ç¨‹æ±‚æ¢¯åº¦
        sgd([w,b],lr,batchSize) #ä½¿ç”¨å‚æ•°çš„æ¢¯åº¦å¯¹å‚æ•°è¿›è¡Œä¸æ–­æ›´æ–°
    with torch.no_grad():
        train_l = loss(net(features,w,b),labels)
        print(f"epoch{epoch + 1 }, loss {float(train_l.mean()):f}")

#epoch1, loss 0.025754
#epoch2, loss 0.000087
#epoch3, loss 0.000047
print(f'wçš„ä¼°è®¡è¯¯å·®: {true_w - w.reshape(true_w.shape)}')
print(f'bçš„ä¼°è®¡è¯¯å·®: {true_b - b}')
wçš„ä¼°è®¡è¯¯å·®: tensor([-0.0002, -0.0004], grad_fn=<SubBackward0>)
bçš„ä¼°è®¡è¯¯å·®: tensor([0.0007], grad_fn=<RsubBackward1>)
```

å¯ä»¥çœ‹å‡ºï¼Œè®­ç»ƒå¾—å‡ºçš„å‚æ•°å’ŒçœŸå®å‚æ•°å·²ç»å¾ˆæ¥è¿‘äº†ã€‚

## åˆ©ç”¨PyTorchç®€æ´å®ç°

åˆ©ç”¨pytorchä¸­å®šä¹‰çš„Linearæ¨¡å‹è¿›è¡Œå®ç°

```python
from importlib.resources import is_resource
import random
import torch
from torch.utils import data
import numpy as np
from d2l import torch as d2l
from line import data_iter

true_w = torch.tensor([2,-3.4])
true_b = 4.2
feature,labels = d2l.synthetic_data(true_w,true_b,1000) #æ„é€ æ•°æ®é›†
def load_array(data_arrays, batch_size, is_train = True):
    """æ„é€ pytorchè¿­ä»£å™¨"""
    dataset = data.TensorDataset(*data_arrays) #è¯»å–æ•°æ®é›†
    return data.DataLoader(dataset,batch_size, shuffle = is_train) 
    #data.DataLoader()æ¯æ¬¡æŒ‘é€‰batch_sizeä¸ªæ ·æœ¬ï¼Œsuffleé€‰æ‹©æ˜¯å¦éšæœºæ‰“ä¹±

batch_size = 10
data_iter = load_array((feature, labels),batch_size)
#next() è¿”å›è¿­ä»£å™¨çš„ä¸‹ä¸€ä¸ªé¡¹ç›®ã€‚
print(next(iter(data_iter)))

#å®šä¹‰æ¨¡å‹
from torch import nn
#æŒ‡å®šè¾“å…¥è¾“å‡ºå±‚
net = nn.Sequential(nn.Linear(2,1)) 
#ç¥ç»ç½‘ç»œç¬¬ä¸€å±‚åˆå§‹åŒ–æ¨¡å‹å‚æ•°
net[0].weight.data.normal_(0,0.01) #w ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ›¿æ¢wçš„å€¼
net[0].bias.data.fill_(0) #b
#å®šä¹‰å‡æ–¹è¯¯å·®
loss = nn.MSELoss() 
#å®ä¾‹åŒ–SGD
#ç¬¬ä¸€ä¸ªå‚æ•°åŒ…æ‹¬netæ‰€æœ‰å‚æ•°ï¼ˆwï¼Œbï¼‰ï¼Œç¬¬äºŒä¸ªå‚æ•°æŒ‡å®šå­¦ä¹ ç‡
trainer = torch.optim.SGD(net.parameters(),lr = 0.03)
num_epoch = 3

for epoch in range(num_epoch):
    for X,y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(feature),labels)
    print(f"epoch{epoch + 1},loss {l:f}")
```


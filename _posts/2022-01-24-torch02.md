---
title: 从零实现线性回归
tags: 深度学习
mathjax: true
mode: immersive
comment: true
pageview: true
key: A0007
header:
  theme: dark
article_header:
  type: cover
  image:
    src: /mountain1.jpg

---


* content
{:toc}


# 线性回归

> 在尝试囫囵吞枣学习深度学习后，我深切地认识到了自己的浮躁，耗费了不少时间，却学到了寥寥无几的零散知识。

## 回归（regression）

研究一组随机变量(Y1 ，Y2 ，…，Yi)和另一组(X1，X2，…，Xk)变量之间关系的统计分析方法，也就是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。当我们需要预测数值时，便可能涉及到回归问题。

## 线性回归

线性回归基于几个简单假设

- 自变量和因变量关系是线性的，也就是因变量y可以表示为自变量x中元素的加权和。（通常可以包含观测值的一些噪声。
- 假设任何噪声都比较正常：噪声符合正态分布。

书中举的例子是房价预测：根据房龄和房间面积估算房屋价值

首先我们需要收集一个真实的数据集，在机器学习中称为**训练集**。训练集中，每行数据被称为**样本**或**数据点**，也就是之前出售过的房屋属性（房龄、面积）。通常，我们把预测的目标（在这个例子里是房价）称为**标签（label）**或者**目标（target）**。把自变量（房龄、面积）称为**特征（feature）**。

线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和。如在这个例子中是

**price=𝑤area⋅area+𝑤age⋅age+𝑏.**

其中𝑤就是权重，它直接决定了每个特征对最终预测值的影响。b称为偏移量（偏置（bias）或截距），当所有特征都取0时，其决定预测值应为多少。

给定训练数据特征**𝐗**和对应的已知标签**𝐲**， 线性回归的目标是找到一组权重向量**𝐰**和偏置𝑏。在开始寻找最好的*模型参数*（model parameters）**𝐰**和𝑏之前， 我们还需要两个东西： 

- 一种模型质量的度量方式(损失函数）； 
- 一种能够更新模型以提高模型预测质量的方法（模型优化）。

## 损失函数

> *损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。回归问题中最常用的损失函数是平方误差函数。

样本l(i) 的平方误差可以定义为 预测值减去真实值的平方除以二。

若度量模型在整体数据集上的质量，我们需要计算训练集n个样本上的损失均值

```
def squared_loss(y_hat,y): #损失函数实现
    #y_hat ： 预测值
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

## 模型优化

解析解：线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解,但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

即使在无法得到解析解的情况，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。书中用到的是梯度下降方法，这个方法几乎可以优化所有深度学习模型。

算法的步骤如下： 

- 初始化模型参数的值，如随机初始化；
- 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。

```
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

## 线性回归从零开始实现

### 数据集生成：

构造人造数据集：

```
import random
from numpy import indices
import torch
from d2l import torch as d2l
def syn(w,b,num_example):              #构造数据集函数
    x = torch.normal(0,1,(num_example,len(w))) #均值为0，方差为1，n个样本，w长度个列
    y = torch.matmul(x,w) + b         #生成真实数据y
    y += torch.normal(0,0.01,y.shape) #像数据中加入均值为0，方差为0.1的随机噪声
    return x,y.reshape((-1,1))

true_w = torch.tensor([2,-3.4])   #真实w
true_b = 4.2                      #真实b
features,labels = syn(true_w,true_b,1000) #生成一千条训练数据，得到featyres，labels
```

### 读取数据集：

```
def data_iter(batch_size,feature,labels):   #批量读取样本
    #打乱数据集中的样本并以小批量方式获取数据
    num_example = len(feature)              #样本数量
    indices = list(range(num_example))      
    random.shuffle(indices)                 #随机读取
    for i in range(0,num_example,batch_size):
        batch_indices = torch.tensor(indices[i:
            min(i + batch_size, num_example)])
        yield feature[batch_indices],labels[batch_indices]

```

### 初始化模型参数：

```
#从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0
w = torch.normal(0,0.01,size = (2,1),requires_grad=True)
b = torch.zeros(1,requires_grad=True) #需要对w和b进行更新，requires_frad = True
#初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。
```

### 定义模型：

```
#线性回归方程
def linreg(X,w,b):
    return torch.matmul(X,w) + b
```

### 定义损失函数：

```
def squared_loss(y_hat,y):
    #y_hat ： 实际值
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

### 定义优化算法：

```
#小批量梯度下降
#param：参数
#lr：学习率
#batch： 步长
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            # print(param.grad)
            param.grad.zero_()
```

### 开始训练：

```
lr = 0.03           #学习率
num_epochs = 3      #训练次数
net = linreg        #回归方程
loss = squared_loss #平方损失函数

for epoch in range(num_epochs): 
    for X,y in data_iter(batchSize,features,labels):
        l = loss(net(X,w,b),y)  
        l.sum().backward() 			#回归方程求梯度
        sgd([w,b],lr,batchSize) #使用参数的梯度对参数进行不断更新
    with torch.no_grad():
        train_l = loss(net(features,w,b),labels)
        print(f"epoch{epoch + 1 }, loss {float(train_l.mean()):f}")

#epoch1, loss 0.025754
#epoch2, loss 0.000087
#epoch3, loss 0.000047
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
w的估计误差: tensor([-0.0002, -0.0004], grad_fn=<SubBackward0>)
b的估计误差: tensor([0.0007], grad_fn=<RsubBackward1>)
```

可以看出，训练得出的参数和真实参数已经很接近了。

## 利用PyTorch简洁实现

利用pytorch中定义的Linear模型进行实现

```python
from importlib.resources import is_resource
import random
import torch
from torch.utils import data
import numpy as np
from d2l import torch as d2l
from line import data_iter

true_w = torch.tensor([2,-3.4])
true_b = 4.2
feature,labels = d2l.synthetic_data(true_w,true_b,1000) #构造数据集
def load_array(data_arrays, batch_size, is_train = True):
    """构造pytorch迭代器"""
    dataset = data.TensorDataset(*data_arrays) #读取数据集
    return data.DataLoader(dataset,batch_size, shuffle = is_train) 
    #data.DataLoader()每次挑选batch_size个样本，suffle选择是否随机打乱

batch_size = 10
data_iter = load_array((feature, labels),batch_size)
#next() 返回迭代器的下一个项目。
print(next(iter(data_iter)))

#定义模型
from torch import nn
#指定输入输出层
net = nn.Sequential(nn.Linear(2,1)) 
#神经网络第一层初始化模型参数
net[0].weight.data.normal_(0,0.01) #w 使用正态分布替换w的值
net[0].bias.data.fill_(0) #b
#定义均方误差
loss = nn.MSELoss() 
#实例化SGD
#第一个参数包括net所有参数（w，b），第二个参数指定学习率
trainer = torch.optim.SGD(net.parameters(),lr = 0.03)
num_epoch = 3

for epoch in range(num_epoch):
    for X,y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(feature),labels)
    print(f"epoch{epoch + 1},loss {l:f}")
```


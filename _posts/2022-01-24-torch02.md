# 从零开始实现线性回归

>在尝试无论吞枣学习深度学习后，我深切地认识到了自己的浮躁，耗费了不少时间，却学到了寥寥无几的零散知识。

## 回归（regression）

研究一组随机变量(Y1 ，Y2 ，…，Yi)和另一组(X1，X2，…，Xk)变量之间关系的统计分析方法，也就是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。当我们需要预测数值时，便可能涉及到回归问题。

## 线性回归

线性回归基于几个简单假设

* 自变量和因变量关系是线性的，也就是因变量y可以表示为自变量x中元素的加权和。（通常可以包含观测值的一些噪声。
* 假设任何噪声都比较正常：噪声符合正态分布。

书中举的例子是房价预测：根据房龄和房间面积估算房屋价值

首先我们需要收集一个真实的数据集，在机器学习中称为**训练集**。训练集中，每行数据被称为**样本**或**数据点**，也就是之前出售过的房屋属性（房龄、面积）。通常，我们把预测的目标（在这个例子里是房价）称为**标签（label）**或者**目标（target）**。把自变量（房龄、面积）称为**特征（feature）**。

线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和。如在这个例子中是

**price=𝑤<sub>area</sub>⋅area+𝑤<sub>age</sub>⋅age+𝑏.**

其中𝑤就是权重，它直接决定了每个特征对最终预测值的影响。b称为偏移量（偏置（bias）或截距），当所有特征都取0时，其决定预测值应为多少。

给定训练数据特征**𝐗**和对应的已知标签**𝐲**， 线性回归的目标是找到一组权重向量**𝐰**和偏置𝑏。在开始寻找最好的*模型参数*（model parameters）**𝐰**和𝑏之前， 我们还需要两个东西： 

* 一种模型质量的度量方式(损失函数）； 
* 一种能够更新模型以提高模型预测质量的方法（模型优化）。

## 损失函数

> *损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。回归问题中最常用的损失函数是平方误差函数。

样本l(i) 的平方误差可以定义为 预测值减去真实值的平方除以二。

若度量模型在整体数据集上的质量，我们需要计算训练集n个样本上的损失均值

```python
def squared_loss(y_hat,y): #损失函数实现
    #y_hat ： 预测值
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

## 模型优化

解析解：线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解,但并不是所有的问题都存在解析解。 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

即使在无法得到解析解的情况，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。书中用到的是梯度下降方法，这个方法几乎可以优化所有深度学习模型。

算法的步骤如下： 

* 初始化模型参数的值，如随机初始化；
* 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。

```python
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

## 线性回归实现

### 数据集生成：

构造人造数据集：

```python
import random
from numpy import indices
import torch
from d2l import torch as d2l
def syn(w,b,num_example):  						 #构造数据集函数
    x = torch.normal(0,1,(num_example,len(w))) #均值为0，方差为1，n个样本，w长度个列
    y = torch.matmul(x,w) + b					#生成真实数据y
    y += torch.normal(0,0.01,y.shape) #像数据中加入均值为0，方差为0.1的随机噪声
    return x,y.reshape((-1,1))

true_w = torch.tensor([2,-3.4]) 	#真实w
true_b = 4.2                    	#真实b
features,labels = syn(true_w,true_b,1000) #生成一千条训练数据，得到featyres，labels
```

读取数据集：

```python 
def data_iter(batch_size,feature,labels):   #批量读取样本
    #打乱数据集中的样本并以小批量方式获取数据
    num_example = len(feature)							#样本数量
    indices = list(range(num_example)) 			
    random.shuffle(indices)                 #随机读取
    for i in range(0,num_example,batch_size):
        batch_indices = torch.tensor(indices[i:
            min(i + batch_size, num_example)])
        yield feature[batch_indices],labels[batch_indices]

```

初始化模型参数：

```python
#从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0
w = torch.normal(0,0.01,size = (2,1),requires_grad=True)
b = torch.zeros(1,requires_grad=True) #需要对w和b进行更新，requires_frad = True
#初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。
```

定义模型：

```python 
#线性回归方程
def linreg(X,w,b):
    return torch.matmul(X,w) + b
```

定义损失函数：

```python
def squared_loss(y_hat,y):
    #y_hat ： 实际值
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

定义优化算法：

```python 
#小批量梯度下降
#param：参数
#lr：学习率
#batch： 步长
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            # print(param.grad)
            param.grad.zero_()
```

开始训练：

```python
lr = 0.03           #学习率
num_epochs = 3      #训练次数
net = linreg        #回归方程
loss = squared_loss #平方损失函数

for epoch in range(num_epochs): 
    for X,y in data_iter(batchSize,features,labels):
        l = loss(net(X,w,b),y)  
        l.sum().backward() 			#回归方程求梯度
        sgd([w,b],lr,batchSize) #使用参数的梯度对参数进行不断更新
    with torch.no_grad():
        train_l = loss(net(features,w,b),labels)
        print(f"epoch{epoch + 1 }, loss {float(train_l.mean()):f}")

#epoch1, loss 0.025754
#epoch2, loss 0.000087
#epoch3, loss 0.000047
```

```python
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
w的估计误差: tensor([-0.0002, -0.0004], grad_fn=<SubBackward0>)
b的估计误差: tensor([0.0007], grad_fn=<RsubBackward1>)
```

可以看出，训练得出的参数和真是参数已经很接近了。

